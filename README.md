Action recognition models are used in robotics, surveillance video and gaming to identify human action in videos. These models are trained using a predefined set of actions and lack the ability to identify actions that were not included in their training dataset. Zero-Shot Action Recognition aims to address this constraint by enabling models to recognize actions that have not been trained. Alternatively, Few-Shot Action Recognition  can recognize actions with a few samples per action class. Existing models of Few-Shot Action Recognition employ meta learning techniques that exhibit limited generalization capabilities when training and testing data are from different domains. Existing models of Zero-Shot Action Recognition employ two modalities, specifically Action Semantic Relation or Action Generative Relation. This paper presents an enhancement to Few-Shot Action Recognition and Zero-Shot Action Recognition through the utilization of a trimodal method, wherein the modalities are processed by the Action Semantics Analyser , Generated Image Similarity Analyser , and Image Action Captioning . This approach allows for a more comprehensive understanding of different domains, thereby enhancing the overall performance of Zero-Shot Action Recognition and Few-Shot Action Recognition models. Caption Semantic Generative Assist improves accuracy of Zero-Shot Action Recognition by +4.97\% for UCF dataset and +5.32\% for HMDB dataset. Caption Semantic Generative Assist improves accuracy of Few-Shot Action Recognition by +2.47\% for UCF dataset and +11.18\% for HMDB dataset.

Source code of the model will be available when the paper is published.
